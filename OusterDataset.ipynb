{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Ouster Data Loading and Processing Tutorial</center></h1> \n",
    "\n",
    "### **Ouster Confidential**\n",
    "\n",
    "**Author:** `Reyhaneh Kazerani`  \n",
    "**Email:** `reyhaneh@ouster.io`   \n",
    "**Date:** Aug 2020   \n",
    "**Version:** 1.0.0  \n",
    "\n",
    "This is an interactive toturial to demonstrate how to load and process Ouster Data for a sample Pytorch model consumption.    \n",
    "\n",
    "First, let's include all the needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance, PILLOW_VERSION\n",
    "from torch.utils.data import * \n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some auxiliary functions for reading a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ouster_frames(path, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads and splits the PNG by their channels, blue (ambient),\n",
    "    green and red (range) and alpha(intensity). Then computes the\n",
    "    range by combining red and green channel.\n",
    "    Args:\n",
    "    path (string) : path to the OSF\n",
    "    Returns:\n",
    "    range (array) : 16 bit integers representing range of each pixel\n",
    "    ambient (array) : memory map of ambient data\n",
    "    intensity (array) : intensity values\n",
    "    \"\"\"\n",
    "    if kwargs.get('cv2', False):\n",
    "        intensity = cv2.imread(path['intensity'], cv2.IMREAD_UNCHANGED)\n",
    "        ambient = cv2.imread(path['ambient'], cv2.IMREAD_UNCHANGED)\n",
    "        range_ = cv2.imread(path['range'], cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.imread(path['mask'], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "\n",
    "        # Channels orders : blue, green, red and alpha\n",
    "        _, intensity_2nd, intensity_1st = cv2.split(intensity)\n",
    "        _, ambient_2nd, ambient_1st = cv2.split(ambient)\n",
    "        range_3rd, range_2nd, range_1st = cv2.split(range_)\n",
    "        \n",
    "        \n",
    "\n",
    "    else:\n",
    "        intensity = Image.open(path['intensity'])\n",
    "        ambient = Image.open(path['ambient'])\n",
    "        range_ = Image.open(path['range'])\n",
    "        mask = Image.open(path['mask']).convert('P')\n",
    "\n",
    "        range_1st, range_2nd, range_3rd = range_.split()\n",
    "\n",
    "    # Calculate range from r and g channels\n",
    "    # Merges the channels\n",
    "    range_ = (np.uint32(range_1st) + (np.uint32(range_2nd) << 8) +\n",
    "           (np.uint32(range_3rd) << 16)) * 1.0\n",
    "\n",
    "    return range_, intensity, ambient, mask\n",
    "\n",
    "\n",
    "def normalize(image , percentile=0.03):\n",
    "    \"\"\"\n",
    "    Scales the image so that contrast is stretched between 0 and 1,\n",
    "    so that the top percentile is 1 and the bottom percentile is 0.\n",
    "    \"\"\"\n",
    "    # Finds the cut off points\n",
    "    flat_arr = image.reshape(-1)\n",
    "    indices = flat_arr.nonzero()[0]\n",
    "    indices_size = indices.shape[0]\n",
    "    kth_extreme = int(percentile*indices_size)\n",
    "    lo = np.partition(flat_arr[indices], kth_extreme)[kth_extreme]\n",
    "    hi = np.partition(flat_arr[indices], indices_size - kth_extreme - 1)[indices_size - kth_extreme - 1]\n",
    "\n",
    "    # Normalizes the image based on the low and high cut offs\n",
    "    image = image.astype(np.float) - lo\n",
    "    image *= (1.0/(hi - lo))\n",
    "    image = image.clip(min=0,max=1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transformations for transforming a data point to tensor, compose different transformations and concatinating different features. These transformations are callable classes instead of simple functions so that parameters of the transform need not be passed everytime itâ€™s called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor():\n",
    "    \"\"\"\n",
    "    Converts an image (PIL) to a tensor\n",
    "    Converts and image in (H x W x C) in the range [0, 255] to a\n",
    "    torch.FloatTensor of shape (C x H x W) in the same range.\n",
    "    The conversion is inplace.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplace=True):\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def __call__(self, data_point, target):\n",
    "        # Checks for inplace or copy\n",
    "        if self.inplace:\n",
    "            new_data_point = data_point\n",
    "            new_target = target\n",
    "        else:\n",
    "            new_data_point = data_point.copy()\n",
    "            new_target = target.copy()\n",
    "        # transforms every channel\n",
    "        for key, value in data_point.items():\n",
    "            new_data_point[key] = to_tensor(value)\n",
    "        # transforms the mask\n",
    "        new_target = torch.as_tensor(np.asarray(target), dtype=torch.int64)\n",
    "    \n",
    "        return new_data_point, new_target\n",
    "    \n",
    "    \n",
    "class FeatureConcat():\n",
    "    \"\"\"\n",
    "    Concatenates Ouster specific features.\n",
    "    If depth feature is used then it casts the tensors to torch.DoubleTensor\n",
    "    features should already be Tensors\n",
    "    Args:\n",
    "     transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "     >>> data_point = {}\n",
    "     >>> data_point['intensity'] = torch.randint(low=0, high=255, (1, 512, 64))\n",
    "     >>> data_point['depth'] = torch.randint(low=0, high=30000, (1, 512, 64), dtype= torch.FloatTensor)\n",
    "     >>> target = torch.randint(low=0, high=10, (1, 512, 64), dtype=torch.LongTensor)\n",
    "     >>> transform = transforms.FeatureConcat(['intensity', 'depth'])\n",
    "     >>> result = transform(data_point, target)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_names=None):\n",
    "        if not feature_names:\n",
    "            self.feature_names = data_point.keys()\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def __call__(self, data_point, target):\n",
    "        # Checks for Tensor type\n",
    "        # FIXME : THIS IS FOR ROS\n",
    "        #assert torch.is_tensor(\n",
    "        #    data_point[self.feature_names[0]]), f\"Pytorch Tensor was expected\"\n",
    "        if len(self.feature_names) == 1:\n",
    "            return data_point[self.feature_names[0]], target\n",
    "        new_data_point = torch.cat(\n",
    "            [data_point[feature].float() for feature in self.feature_names], dim=0)\n",
    "        return new_data_point, target\n",
    "    \n",
    "\n",
    "class Compose():\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    Args:\n",
    "      transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "      >>> transforms.Compose([\n",
    "      >>>     transforms.ToTensor(),\n",
    "      >>>     transforms.FeatureConcat(['features'])\n",
    "      >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data_point, target):\n",
    "        for transform in self.transforms:\n",
    "            data_point, target = transform(data_point, target)\n",
    "        return data_point, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for transform in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(transform)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "    \n",
    "    \n",
    "def to_tensor(pic):\n",
    "    \"\"\"\n",
    "    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    See ``ToTensor`` for more details.\n",
    "    Args:\n",
    "     pic (PIL Image or numpy.ndarray): osf feature (like intensity, depth or ambient) to be converted to tensor.\n",
    "    Returns:\n",
    "     Tensor: Converted osf feature.\n",
    "    \"\"\"\n",
    "    if isinstance(pic, np.ndarray):\n",
    "        # handle numpy array\n",
    "        if pic.ndim == 2:\n",
    "            pic = pic[:, :, None]\n",
    "\n",
    "        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "        # backward compatibility\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float()  #.div(255)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    # handle PIL Image\n",
    "    if pic.mode == 'I':\n",
    "        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "    elif pic.mode == 'I;16':\n",
    "        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "    elif pic.mode == 'F':\n",
    "        img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n",
    "    elif pic.mode == '1':\n",
    "        img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))\n",
    "    else:\n",
    "        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "    # PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "    if pic.mode == 'YCbCr':\n",
    "        nchannel = 3\n",
    "    elif pic.mode == 'I;16':\n",
    "        nchannel = 1\n",
    "    else:\n",
    "        nchannel = len(pic.mode)\n",
    "    img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "    # put it from HWC to CHW format\n",
    "    # yikes, this transpose takes 80% of the loading time/CPU\n",
    "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "    if isinstance(img, torch.ByteTensor):\n",
    "        return img.float()  #.div(255)\n",
    "    else:\n",
    "        return img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OusterDataset(Dataset):\n",
    "    def __init__(self, data_dir, features, transforms=None):\n",
    "        # Sets the data directory\n",
    "        self.data_dir= data_dir\n",
    "        self.ambient_ls = list(\n",
    "          sorted(os.listdir(os.path.join(data_dir, \"ambient\"))))\n",
    "        self.intensity_ls = list(\n",
    "          sorted(os.listdir(os.path.join(data_dir, \"intensity\"))))\n",
    "        self.range_ls = list(\n",
    "          sorted(os.listdir(os.path.join(data_dir, \"range\"))))\n",
    "        self.masks_ls = list(\n",
    "          sorted(os.listdir(os.path.join(data_dir, \"semantic_masks\"))))\n",
    "\n",
    "        #  Sets the transforms\n",
    "        self.transforms = self.get_transforms(\n",
    "          transforms, features=features)  # Preprocessing for data\n",
    "\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        intensity = os.path.join(os.path.join(self.data_dir, \"intensity\"),\n",
    "                               self.intensity_ls[idx])\n",
    "        range_ = os.path.join(os.path.join(self.data_dir, \"range\"), self.range_ls[idx])\n",
    "        ambient = os.path.join(os.path.join(self.data_dir, \"ambient\"), self.ambient_ls[idx])\n",
    "        mask = os.path.join(self.data_dir, \"semantic_masks\", self.masks_ls[idx])\n",
    "        frame_path = {\n",
    "            'intensity': intensity,\n",
    "            'ambient': ambient,\n",
    "            'range': range_,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Reading the osf and its mask\n",
    "        range_, intensity, ambient, mask = read_ouster_frames(frame_path)\n",
    "       \n",
    "\n",
    "        data_point = {}\n",
    "        data_point[\"intensity\"] = intensity\n",
    "        data_point[\"range\"] = range_\n",
    "        data_point[\"ambient\"] = ambient\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            data_point, mask = self.transforms(data_point, mask)\n",
    "        return {'data': data_point, 'mask':mask}\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.intensity_ls)\n",
    " \n",
    "    @staticmethod\n",
    "    def get_transforms(train=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Composes the transforms. If dataset is not in train mode then it dwould\n",
    "        only do the primary transfomation for network usage.\n",
    "        Args:\n",
    "          train (boolean) : in train mode or not\n",
    "          transforms (optional, Callable): list of transfomations\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        transforms = [] if not train else kwargs('transforms', [])\n",
    "        transforms.append(ToTensor())\n",
    "        transforms.append(FeatureConcat(kwargs.get('features', None)))\n",
    "        return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize our data and see if it has been loaded currectly. for that you can easily enter the path to your data folder below. Choose an index in range of your data length, and run the following commands. You can easily choose which features to be added by changing the feature parameter `feature` in `OusterDataset`. You can choose any subset of `intensity`, `ambient` and `range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data' # Path to data folder\n",
    "idx = 0 # Index for a data point\n",
    "features = ['intensity', 'range', 'ambient'] # A list of features to be processed\n",
    "\n",
    "# Instantiates an instance of OusterDataset for reading and processing the data\n",
    "test_data = OusterDataset(data, features=features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the data you need to reformat the tensors to `numpy` arrays. You can follow the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the tensors to numpy objects\n",
    "intensity=test_data[idx]['data'][0,:,:].cpu().numpy()\n",
    "range_=test_data[idx]['data'][1,:,:].cpu().numpy() \n",
    "ambient=test_data[idx]['data'][2,:,:].cpu().numpy()\n",
    "mask=test_data[idx]['mask'].cpu().numpy()\n",
    "\n",
    "# Visualizes the results\n",
    "f_image, ax_image = plt.subplots(4, figsize=(15, 5))\n",
    "ax_image[0].imshow(normalize(intensity))\n",
    "ax_image[0].set_title('Intensity image')\n",
    "ax_image[1].imshow(normalize(range_))\n",
    "ax_image[1].set_title('Range image' )\n",
    "ax_image[2].imshow(normalize(ambient))\n",
    "ax_image[2].set_title('Ambient image')\n",
    "ax_image[3].imshow(normalize(mask))\n",
    "ax_image[3].set_title('Semantic mask')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
